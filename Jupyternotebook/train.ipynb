{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ak/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 128,128 # Dimension of the image\n",
    "VGG = VGG16(weights='imagenet', include_top=False, input_shape=(img_rows, img_cols,3))\n",
    "\n",
    "# include_top=False is excluding he top layer of VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layers in VGG.layers:\n",
    "    layers.trainable = False \n",
    "    \n",
    "# Since we are going to train the last of the VGG16 so that we are ignoring the first and training only the last layyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the last layer of VGG\n",
    "def addTopModelVGG(bottom_model, num_classes):\n",
    "    top_model = bottom_model.output\n",
    "    top_model = GlobalAveragePooling2D()(top_model)\n",
    "    top_model = Dense(512,activation='relu')(top_model) # 512 neurons and relu activation\n",
    "    top_model = Dense(256,activation='relu')(top_model) # 256 neurons and relu activation\n",
    "    top_model = Dense(256,activation='relu')(top_model) # 256 neurons and relu activation\n",
    "    top_model = Dense(128,activation='relu')(top_model) # 128 neurons and relu activation\n",
    "    top_model = Dense(128,activation='relu')(top_model) # 128 neurons and relu activation\n",
    "    top_model = Dense(num_classes,activation='softmax')(top_model) # softmax activation\n",
    "    return top_model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 15,224,646\n",
      "Trainable params: 509,958\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ak/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "num_classes = 6\n",
    "FC_Head = addTopModelVGG(VGG, num_classes) # Calling addTopModelVGG function\n",
    "# Parameters indicate the bottom model and the number of classes\n",
    "model = Model(inputs = VGG.input, output = FC_Head)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data directory\n",
    "train_data_dir = 'train' \n",
    "# Validation data directory\n",
    "validation_data_dir = 'validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genertation more images from a single image\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   rotation_range = 30,\n",
    "                                   width_shift_range = 0.3,\n",
    "                                   height_shift_range = 0.3,\n",
    "                                   horizontal_flip = True,\n",
    "                                   fill_mode = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Cross checking the training data\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size indicates the number of images to be traimed at a time\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# More features to be added to the training data\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                   target_size = (img_rows, img_cols),\n",
    "                                                   batch_size = batch_size,\n",
    "                                                   class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                             target_size = (img_rows, img_cols),\n",
    "                                                             batch_size = batch_size,\n",
    "                                                             class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelCheckpoint - Save only those with the best accuracy\n",
    "checkpoint = ModelCheckpoint('finger_sign.h5',\n",
    "                            monitor = 'val_loss', # It monitors loss or not\n",
    "                            mode = 'min',\n",
    "                            save_best_only = True, # Save only the best \n",
    "                            verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping - If model validation is not improving then we stop\n",
    "earlystop = EarlyStopping(monitor = 'val_loss',\n",
    "                         min_delta = 0,\n",
    "                         restore_best_weights = True,\n",
    "                         patience = 10, # If patience = 3 means that the model validation doesn't increase for 10 rounds it stop\n",
    "                         verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReduceLROnPlateau - Reduce learning rate. If the model accuracy is not improving reduce the learning rate\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                                            patience = 5, # If patience = 3 means that the model validation doesn't increase for 5 rounds it reduce the learning rate\n",
    "                                             verbose = 1,\n",
    "                                             factor = 0.2,\n",
    "                                             min_lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [earlystop, checkpoint, learning_rate_reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = Adam(lr = 0.001),\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_samples = 1200\n",
    "nb_validaton_samples = 300\n",
    "\n",
    "epochs = 10 # Number of times want to be trained\n",
    "batch_size = 32 # number of images to be traimed at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ak/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "37/37 [==============================] - 347s 9s/step - loss: 1.3909 - accuracy: 0.3964 - val_loss: 1.4305 - val_accuracy: 0.5833\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.43050, saving model to finger_sign.h5\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 373s 10s/step - loss: 0.6462 - accuracy: 0.7252 - val_loss: 1.3324 - val_accuracy: 0.3470\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.43050 to 1.33244, saving model to finger_sign.h5\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 412s 11s/step - loss: 0.2769 - accuracy: 0.9050 - val_loss: 1.0390 - val_accuracy: 0.5485\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.33244 to 1.03899, saving model to finger_sign.h5\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 470s 13s/step - loss: 0.2102 - accuracy: 0.9240 - val_loss: 1.6937 - val_accuracy: 0.6754\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.03899\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 460s 12s/step - loss: 0.1210 - accuracy: 0.9575 - val_loss: 3.5099 - val_accuracy: 0.6269\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.03899\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 563s 15s/step - loss: 0.0896 - accuracy: 0.9688 - val_loss: 2.6429 - val_accuracy: 0.6381\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.03899\n",
      "Epoch 7/10\n",
      "37/37 [==============================] - 463s 13s/step - loss: 0.1073 - accuracy: 0.9661 - val_loss: 1.7110 - val_accuracy: 0.5896\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.03899\n",
      "Epoch 8/10\n",
      "37/37 [==============================] - 474s 13s/step - loss: 0.0532 - accuracy: 0.9820 - val_loss: 3.2889 - val_accuracy: 0.4925\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.03899\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 9/10\n",
      "37/37 [==============================] - 472s 13s/step - loss: 0.0327 - accuracy: 0.9890 - val_loss: 1.7771 - val_accuracy: 0.5485\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.03899\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 468s 13s/step - loss: 0.0312 - accuracy: 0.9887 - val_loss: 3.5280 - val_accuracy: 0.5597\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.03899\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                             steps_per_epoch = nb_train_samples//batch_size,\n",
    "                             epochs = epochs,\n",
    "                             callbacks = callbacks,\n",
    "                             validation_data = validation_generator,\n",
    "                             validation_steps = nb_validaton_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
